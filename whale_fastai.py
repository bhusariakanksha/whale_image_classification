# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12rLFmgMh_9eXpxqtrc_a1pwGssvXhAEZ
"""

####
# fast ai classifier
####

from fastai.vision import *
from fastai.metrics import *

import numpy as np
from fastai import *
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

import glob
from pathlib import Path
import re
import os

from google.colab import drive
drive.mount('/content/drive')

#first, only consider whales with ID's... exclude any 'new whale' values
#determine number of classes, define class list
#make image list and key list

model_path = '/content/drive/My Drive/whale_models/'
learn_import = load_learner(model_path, 'export.pkl')

type(learn_import)

path = Path('/content/drive/My Drive/train_unzipped/train/')

img_filename_list = os.listdir(path)

key_path = Path('/content/drive/My Drive/train_key.csv/')



import random

key_csv = pd.read_csv(key_path)
#print(key_csv[key_csv['Id']=='new_whale'])


labels = []
for i in range(len(key_csv)):
  rand = random.uniform(0, 1)
  if rand > .2:
    labels.append("True")
  if rand < .2:
    labels.append("False")

key_csv['is_valid']=labels
print(key_csv)

key_csv_no_new_whales = key_csv[key_csv['Id']!='new_whale']

img_file_list = []
whale_id_list = []
for i in range(len(key_csv_no_new_whales.index)):
  img_file_list.append(key_csv_no_new_whales.iloc[i,0])
  whale_id_list.append(key_csv_no_new_whales.iloc[i,1])

full_img_path_list = []
for i in range(len(key_csv_no_new_whales.index)):
  part = '/content/drive/My Drive/train_unzipped/train/'
  ending = img_file_list[i]
  full_img_path_list.append(part+ending)

len(full_img_path_list)

dls = ImageDataLoaders.from_df(key_csv, path, folder='train', valid_col='is_valid', label_delim=' ',
                               item_tfms=Resize(460), batch_tfms=aug_transforms(size=224))

# from fastai import ImageDataLoaders
path = '/content/drive/My Drive/train_unzipped/train/'


#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


#full_img_path_list, whale_id_list = full_img_path_list.to(device), whale_id_list.to(device)

tfms = get_transforms(do_flip=True,max_rotate=180.0,max_lighting=0.2,max_warp=0.2,p_affine=0.75,p_lighting=0.75)
#data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=224, bs=16).normalize(imagenet_stats)


dls = ImageDataBunch.from_lists(path, full_img_path_list, whale_id_list, size = (384,384), ds_tfms=tfms, no_check = True)#.normalize(imagenet_stats)

#dls = ImageDataBunch.from_df(path, full_img_path_list, whale_id_list, size = (384,384,1), ds_tfms=get_transforms(), no_check = True)#.normalize(imagenet_stats)
#dls = ImageDataBunch.from_df(key_csv, path, folder='train', valid_col='is_valid', label_delim=' ', size = (384,384,1)) #, batch_tfms=aug_transforms(size=224)
#1100,500 

# do batch tfms and add in image resizing (done)

dls.show_batch()

# for data Augmentation some of the most imporant parameters to adjust are do_flip and flip_vert 
# some other are  zoom, shift, rotation and shear (img.do)

# do_flip: True applies random flip
# flip_vert: False = limits the horizontal flips, True = limits vertical and rotations
# Image_trans = get_transforms(mult=1.0, do_flip=True, flip_vert=False, max_rotate=10.0, min_zoom=1.0, max_zoom=1.1, 
#                              max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75, xtra_tfms=None, size=None, 
#                              mode='bilinear', pad_mode='reflection', align_corners=True, batch=False, min_scale=1.0)

# img = open_image('/content/drive/My Drive/train_unzipped/train/00022e1a.jpg')
# img

img_sizes = []
for i in range(30):
  img_file = open_image(full_img_path_list[i])
  img_sizes.append(img_file.data.shape)

#img.data.shape

img_sizes

print('##########################')
print('##########################')
print(dls.classes)
print(len(dls.classes))
print('##########################')
print('##########################')

np.random.seed(42)




setattr(dls, 'device', 'cuda')
model = models.resnet34

# loss function probably gets specified in this line below
learn = cnn_learner(dls, model, callback_fns=ShowGraph, metrics=[accuracy],pretrained=True)
learn.model

learn.fit_one_cycle(20)


### use cross entropy loss

learn.path = Path('/content/drive/My Drive/whale_models/')
learn.export()

learn.export(file = Path("/content/drive/My Drive/whale_models/export_exportmethod.pkl"))
#learn = load_learner(deployed_path)
learn.save(file = Path("/content/drive/My Drive/whale_models/export_savemethod.pkl"))

# learn.export

# model_safe_path = Path('/content/drive/My Drive/')
# learn.export.model_safe_path

# learn_temp = open('/content/drive/My Drive/whale_models/export.pkl', 'rb')
# learn = pickle.load(learn_temp)
# learn_temp.close()


#learn = pickle.load(open('/content/drive/My Drive/whale_models/export.pkl','rb'))
# with open('/content/drive/My Drive/whale_models/export.pkl', 'rb') as f:
#     learn = pickle.load(f)

testing_folder_path = Path('/content/drive/My Drive/whale_test/')
test_list_end = os.listdir(testing_folder_path)


full_test_path_list = []
testing_folder_path_str = '/content/drive/My Drive/whale_test/'
for i in range(len(test_list_end)):
  full_test_path_list.append(testing_folder_path_str+test_list_end[i])

test_list_end[:5]

#model_path = '/content/drive/My Drive/whale_models/'
#learn_import = load_learner(model_path)

#learner = torch.load('/content/drive/My Drive/whale_models/export.pkl')

#type(learn_import)

path = '/content/drive/My Drive/whale_models/'
learn = load_learner(path, 'export.pkl')

te = full_img_path_list[1]

test_img = open_image(te)
test_pred = learn.predict(test_img)

print(test_pred)

len(full_test_path_list)

first_third_test = full_test_path_list[:5000]
second_third_test = full_test_path_list[5000:10000]
third_third_test = full_test_path_list[10000:]
print(len(first_third_test)+len(second_third_test)+len(third_third_test))

predictions_list = []

for i in first_third_test:
  temp_img = open_image(i)
  temp_prediction = learn.predict(temp_img)
  predictions_list.append(temp_prediction[2])

#preds_df = learn.pred_batch(full_test_path_list)

#preds_df

whale_ids = dls.classes

pred_tuple_list = []
for i in predictions_list:
  temp_list = []
  for j in range(len(i)):
    x = tuple((whale_ids[j], i[j]))
    temp_list.append(x)
  pred_tuple_list.append(temp_list)

combined_list = []
for i in pred_tuple_list:
  t = sorted(i, key=lambda x: x[1], reverse = True)
  tt = t[:4]
  # new_whale="whale"
  # tt.append(new_whale)
  combined_list.append(tt)
  

final_list = []
for i in combined_list:
  t_list = []
  for j in i:
    t_list.append(j[0])
  final_list.append(t_list)

final_list1 = []
for i in final_list:
  str_element = ' '.join([str(elem) for elem in i])
  str_element = str_element + ' new_whale'
  final_list1.append(str_element)



d = {'Image':test_list_end[:5000],'Id':final_list1}
kaggle_df = pd.DataFrame(d)


# kaggle_df['Id'] = kaggle_df['Id'].str.replace('[', '')
# kaggle_df['Id'] = kaggle_df['Id'].str.replace(']', '')
# kaggle_df['Id'] = kaggle_df['Id'].str.replace(',', '')

print(kaggle_df)

kaggle_df.to_csv(r'/content/drive/My Drive/whale_submission_files/kaggle_submission_w_new_whale_first_third.csv',index=False)

whale_ids = dls.classes

pred_tuple_list = []
for i in predictions_list:
  temp_list = []
  for j in range(len(i)):
    x = tuple((whale_ids[j], i[j]))
    temp_list.append(x)
  pred_tuple_list.append(temp_list)

combined_list = []
for i in pred_tuple_list:
  t = sorted(i, key=lambda x: x[1], reverse = True)
  tt = t[:5]
  # new_whale="whale"
  # tt.append(new_whale)
  combined_list.append(tt)
  

final_list = []
for i in combined_list:
  t_list = []
  for j in i:
    t_list.append(j[0])
  final_list.append(t_list)

final_list1 = []
for i in final_list:
  str_element = ' '.join([str(elem) for elem in i])
  #str_element = str_element + ' new_whale'
  final_list1.append(str_element)



d2 = {'Image':test_list_end,'Id':final_list1}
kaggle_df2 = pd.DataFrame(d2)


# kaggle_df['Id'] = kaggle_df['Id'].str.replace('[', '')
# kaggle_df['Id'] = kaggle_df['Id'].str.replace(']', '')
# kaggle_df['Id'] = kaggle_df['Id'].str.replace(',', '')

#print(kaggle_df2)

kaggle_df2.to_csv(r'/content/drive/My Drive/whale_submission_files/kaggle_submission_no_new_whale.csv',index=False)

print(len(test_list_end))
print(len(final_list))
print(final_list[5][4])

print(final_list[1])
print(combined_list[1])

print(kaggle_df)

kaggle_df

#learn.save('/content/drive/My Drive/whale_models/dec10_model.pkl')

from fastai.vision import *

path = '/content/drive/My Drive/whale_models/'
learn_import = load_learner(path, 'export.pkl')

test_pred = learn_import.predict(test_img)

test_pred

